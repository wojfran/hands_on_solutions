{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import exp, log\n",
    "import winsound\n",
    "from sklearn.discriminant_analysis import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir(date, name):\n",
    "    run_id = time.strftime(date)\n",
    "    return os.path.join(root_logdir, run_id, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3072)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = keras.datasets.cifar10\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1, 32 * 32 * 3)\n",
    "test_images = test_images.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_images = scaler.fit_transform(train_images)\n",
    "test_images = scaler.transform(test_images)\n",
    "\n",
    "one_hot = np.zeros((len(train_labels), 10))\n",
    "one_hot_test = np.zeros((len(test_labels), 10))\n",
    "\n",
    "for i, label in enumerate(train_labels):\n",
    "    one_hot[i][label] = 1\n",
    "\n",
    "for i, label in enumerate(test_labels):\n",
    "    one_hot_test[i][label] = 1\n",
    "\n",
    "train_labels = one_hot\n",
    "test_labels = one_hot_test\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2)\n",
    "\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAlphaDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate=0.1, noise_shape=None, seed=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom AlphaDropout layer that applies dropout for SELU activations.\n",
    "\n",
    "        Args:\n",
    "            rate (float): Dropout rate (between 0 and 1).\n",
    "            noise_shape (1D tensor of int or None): 1D shape tuple representing the \n",
    "                shape of the binary dropout mask that will be multiplied with \n",
    "                the input. For instance, if input shape is (None, 32, 32, 3) \n",
    "                and noise_shape is (32, 32, 1), the dropout mask will be \n",
    "                broadcasted to (None, 32, 32, 3).\n",
    "            seed (int or None): Random seed for reproducibility.\n",
    "            **kwargs: Additional keyword arguments for the Layer.\n",
    "        \"\"\"\n",
    "        super(CustomAlphaDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        self.alpha = 1.6732632423543772848170429916717 \n",
    "        self.scale = 1.0507009873554804934193349852946 \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        super(CustomAlphaDropout, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Ensure training is a Tensor\n",
    "        training = tf.convert_to_tensor(training, dtype=tf.bool)\n",
    "\n",
    "        def dropped_inputs():\n",
    "            # Determine the noise shape\n",
    "            noise_shape = self.noise_shape or tf.shape(inputs)\n",
    "            random_tensor = tf.random.uniform(noise_shape, seed=self.seed)\n",
    "            keep_prob = 1.0 - self.rate\n",
    "            \n",
    "            # Create binary tensor\n",
    "            binary_tensor = tf.cast(random_tensor >= self.rate, inputs.dtype)\n",
    "            alpha_dropout_scale = self.scale * (binary_tensor + keep_prob - 1.0)\n",
    "            alpha_dropout_shift = self.alpha * (1.0 - binary_tensor)\n",
    "\n",
    "            # Apply the dropout and the affine transformation\n",
    "            return inputs * alpha_dropout_scale + alpha_dropout_shift\n",
    "\n",
    "        # Use tf.cond to switch between training and inference modes\n",
    "        return tf.cond(training, dropped_inputs, lambda: inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"rate\": self.rate,\n",
    "            \"noise_shape\": self.noise_shape,\n",
    "            \"seed\": self.seed,\n",
    "        }\n",
    "        base_config = super(CustomAlphaDropout, self).get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        self.model.optimizer.learning_rate.assign(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type corresponds to the type of model to be built as per exercise instructions\n",
    "# each type consists of 20 dense layers with 100 neurons each with Nadam optimizer\n",
    "# type 0: HE initialization, ELU activation\n",
    "# type 1: HE initialization, ELU activation, Batch Normalization\n",
    "# type 2: Lecun initialization, SELU activation\n",
    "# type 3: Lecun initialization, SELU activation, AlphaDropout\n",
    "\n",
    "def build_model(learning_rate=3e-3, type=0):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(3072,)))\n",
    "\n",
    "    if type == 0:\n",
    "        for i in range(20):\n",
    "            model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "    elif type == 1:\n",
    "        for i in range(20):\n",
    "            model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "    elif type == 2:\n",
    "        for i in range(20):\n",
    "            model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "    elif type == 3:\n",
    "        for i in range(20):\n",
    "            model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "            model.add(CustomAlphaDropout(rate=0.05))\n",
    "    else:\n",
    "        print(\"Invalid type\")\n",
    "        return\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(keras.callbacks.Callback):\n",
    "    def __init__(self, start_lr=1e-5, end_lr=1e-1, num_iterations=50, max_loss=300):\n",
    "        super().__init__()\n",
    "        self.start_lr = start_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        self.iteration = 0\n",
    "        self.multiplier = (end_lr/start_lr)**(1/num_iterations)\n",
    "        self.max_loss = max_loss\n",
    "        self.optimal_lr = None\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        lr = self.start_lr * (self.multiplier ** self.iteration)\n",
    "        self.model.optimizer.learning_rate.assign(lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        loss = logs['loss']\n",
    "        if not np.isnan(loss) and loss < self.max_loss:\n",
    "            self.losses.append(loss)\n",
    "        else:\n",
    "            self.losses.append(None)\n",
    "        self.iteration += 1\n",
    "        if self.iteration >= self.num_iterations:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def plot_loss(self):\n",
    "        valid_losses = [loss for loss in self.losses if loss is not None]\n",
    "        valid_learning_rates = [lr for lr, loss in zip(self.learning_rates, self.losses) if loss is not None]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(valid_learning_rates, valid_losses)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate vs. Loss')\n",
    "        \n",
    "        min_loss_idx = np.argmin(valid_losses)\n",
    "        \n",
    "        optimal_lr = valid_learning_rates[min_loss_idx] / 10\n",
    "        \n",
    "        plt.axvline(x=optimal_lr, color='red', linestyle='--', label=f'Optimal LR: {optimal_lr:.2e}')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def get_optimal_lr(self):\n",
    "        valid_losses = [loss for loss in self.losses if loss is not None]\n",
    "        valid_learning_rates = [lr for lr, loss in zip(self.learning_rates, self.losses) if loss is not None]\n",
    "        \n",
    "        min_loss_idx = np.argmin(valid_losses)\n",
    "        \n",
    "        optimal_lr = valid_learning_rates[min_loss_idx] / 10\n",
    "\n",
    "        return optimal_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModelCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='val_loss', mode='min'):\n",
    "        super(BestModelCallback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_weights = None\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "\n",
    "        if (self.mode == 'min' and current < self.best) or (self.mode == 'max' and current > self.best):\n",
    "            self.best = current\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weights is not None:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            print(f\"\\nRestored model to best {self.monitor} from epoch {self.best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\ml\\my_env\\lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restored model to best val_loss from epoch 8\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "HE_ELU - Test Accuracy: 41.81%\n",
      "\n",
      "Restored model to best val_loss from epoch 17\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "HE_ELU_OneCycle - Test Accuracy: 49.37%\n",
      "\n",
      "Restored model to best val_loss from epoch 25\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
      "HE_ELU_BATCH - Test Accuracy: 53.23%\n",
      "\n",
      "Restored model to best val_loss from epoch 21\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "HE_ELU_BATCH_OneCycle - Test Accuracy: 53.30%\n",
      "\n",
      "Restored model to best val_loss from epoch 10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "LECUN_SELU - Test Accuracy: 46.56%\n",
      "\n",
      "Restored model to best val_loss from epoch 22\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "LECUN_SELU_OneCycle - Test Accuracy: 51.78%\n",
      "\n",
      "Restored model to best val_loss from epoch 17\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step\n",
      "LECUN_SELU_ALPHA0.05 - Test Accuracy: 40.46%\n",
      "\n",
      "Restored model to best val_loss from epoch 27\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step\n",
      "LECUN_SELU_ALPHA0.05_OneCycle - Test Accuracy: 51.97%\n"
     ]
    }
   ],
   "source": [
    "date = time.strftime(\"%Y_%m_%d-%H_%M\")\n",
    "names = [\"HE_ELU\", \"HE_ELU_BATCH\", \"LECUN_SELU\", \"LECUN_SELU_ALPHA0.05\", \n",
    "         \"HE_ELU_OneCycle\", \"HE_ELU_BATCH_OneCycle\", \"LECUN_SELU_OneCycle\", \"LECUN_SELU_ALPHA0.05_OneCycle\"]\n",
    "models = {}\n",
    "models1cycle = {}\n",
    "batch_size = 32\n",
    "epochs = 32\n",
    "\n",
    "for i in range(0, 4):\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)\n",
    "    best_model_callback = BestModelCallback(monitor='val_loss', mode='min')\n",
    "    for j in range(0,2):\n",
    "        if j==0:\n",
    "            run_logdir = get_run_logdir(date, names[i])\n",
    "            tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "            lr_finder = LRFinder(start_lr=1e-5, end_lr=10, num_iterations=5000)\n",
    "            model = build_model(type=i)\n",
    "            model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels), batch_size=batch_size,\n",
    "                    callbacks=[lr_finder], verbose=0)\n",
    "            optimal_lr = lr_finder.get_optimal_lr()\n",
    "            model = build_model(learning_rate=optimal_lr, type=i)\n",
    "            model.fit(train_images, train_labels, epochs=epochs, validation_data=(val_images, val_labels), batch_size=batch_size,\n",
    "                    callbacks=[tensorboard_cb, best_model_callback], verbose=0)\n",
    "            models[i] = model\n",
    "        else:\n",
    "            run_logdir = get_run_logdir(date, names[i+4])\n",
    "            tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "            scheduler = OneCycleScheduler(iterations=epochs*(train_images.shape[0]/batch_size),start_rate=1e-5, max_rate=1e-3)\n",
    "            model = build_model(type=i)\n",
    "            model.fit(train_images, train_labels, epochs=epochs, validation_data=(val_images, val_labels), batch_size=batch_size,\n",
    "                    callbacks=[scheduler, tensorboard_cb, best_model_callback], verbose=0)\n",
    "            models1cycle[i] = model\n",
    "       \n",
    "        predictions = model.predict(test_images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(test_labels, axis=1)\n",
    "        accuracy = np.mean(predicted_classes == true_classes)\n",
    "\n",
    "        print(f\"{names[i+j*4]} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        with open(f\"{run_logdir}/accuracy.txt\", \"a\") as f:\n",
    "            f.write(f\"{names[i+j*4]} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        model.save(f\"{run_logdir}/{names[i+j*4]}.keras\")\n",
    "        \n",
    "\n",
    "winsound.Beep(2500, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction without MC Dropout\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "[[0.01 0.03 0.12 0.12 0.09 0.08 0.5  0.03 0.01 0.01]]\n",
      "Prediction with MC Dropout Mean\n",
      "[[0.03 0.03 0.17 0.13 0.14 0.15 0.27 0.06 0.01 0.02]]\n",
      "MC Dropout Test Accuracy: 41.31%\n",
      "MC Dropout Test Accuracy with 1Cycle model: 52.57%\n"
     ]
    }
   ],
   "source": [
    "## Test the accuracy of model 4 with mc dropout\n",
    "y_probas = np.stack([models[3](test_images, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "\n",
    "print(\"Prediction without MC Dropout\")\n",
    "print(np.round(models[3].predict(test_images[:1]), 2))\n",
    "\n",
    "print(\"Prediction with MC Dropout Mean\")\n",
    "print(np.round(y_proba[:1], 2))\n",
    "\n",
    "accuracy = np.mean(np.argmax(y_proba, axis=1) == np.argmax(test_labels, axis=1))\n",
    "\n",
    "get_run_logdir(date, names[3])\n",
    "\n",
    "print(f\"MC Dropout Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "with open(f\"{run_logdir}/accuracy.txt\", \"a\") as f:\n",
    "        f.write(f\"MC dropout - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "y_probas = np.stack([models1cycle[3](test_images, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "\n",
    "accuracy = np.mean(np.argmax(y_proba, axis=1) == np.argmax(test_labels, axis=1))\n",
    "\n",
    "get_run_logdir(date, names[7])\n",
    "\n",
    "with open(f\"{run_logdir}/accuracy.txt\", \"a\") as f:\n",
    "        f.write(f\"1Cycle MC dropout  - Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(f\"MC Dropout Test Accuracy with 1Cycle model: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
