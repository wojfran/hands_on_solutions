{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "FILENAME = \"aclImdb_v1.tar.gz\"\n",
    "DATASET_PATH = Path(keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=False)).parent / \"aclImdb\"\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
    "else:\n",
    "    print(\"Dataset already downloaded.\")\n",
    "\n",
    "path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath, encoding=\"utf8\") as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels)))\n",
    "\n",
    "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print()\n",
    "\n",
    "raw_train_ds = imdb_dataset(train_pos, train_neg).shuffle(buffer_size=(len(train_neg) + len(train_pos)))\n",
    "raw_valid_ds = imdb_dataset(valid_pos, valid_neg).shuffle(buffer_size=(len(train_neg) + len(train_pos)))\n",
    "raw_test_ds = imdb_dataset(test_pos, test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "def remove_html(text):\n",
    "    html_tag_pattern = r'<.*?>'\n",
    "    text = tf.strings.regex_replace(text, html_tag_pattern, '')\n",
    "    return text\n",
    "\n",
    "@tf.function\n",
    "def tf_standardize(text):\n",
    "    text_no_html = remove_html(text) \n",
    "    text_standardized = tf.strings.regex_replace(text_no_html, r\"[^\\w\\s]\", \"\")\n",
    "    text_standardized = tf.strings.lower(text_standardized)\n",
    "    return text_standardized\n",
    "\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=tf_standardize\n",
    "    )\n",
    "\n",
    "text_ds = imdb_dataset(train_pos, train_neg).map(lambda x, y: x)\n",
    "\n",
    "vectorization_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'Judy Holliday struck gold in 1950 withe George Cukor\\'s film version of \"Born Yesterday,\" and from that point forward, her career consisted of trying to find material good enough to allow her to strike gold again.<br /><br />It never happened. In \"It Should Happen to You\" (I can\\'t think of a blander title, by the way), Holliday does yet one more variation on the dumb blonde who\\'s maybe not so dumb after all, but everything about this movie feels warmed over and half hearted. Even Jack Lemmon, in what I believe was his first film role, can\\'t muster up enough energy to enliven this recycled comedy. The audience knows how the movie will end virtually from the beginning, so mostly it just sits around waiting for the film to catch up.<br /><br />Maybe if you\\'re enamored of Holliday you\\'ll enjoy this; otherwise I wouldn\\'t bother.<br /><br />Grade: C', shape=(), dtype=string)\n",
      "Label tf.Tensor(0, shape=(), dtype=int32)\n",
      "Vectorized review (<tf.Tensor: shape=(250,), dtype=int64, numpy=\n",
      "array([4381,    1, 3524, 1942,    8, 6393,    1,  729,    1,   19,  302,\n",
      "          5, 1483, 4097,    3,   36,   12,  213,  963,   39,  615, 9223,\n",
      "          5,  256,    6,  158,  806,   49,  188,    6, 1671,   39,    6,\n",
      "       3492, 1942,    1,  109,  559,    8,    9,  138,  588,    6,   23,\n",
      "         11,  172,  102,    5,    4,    1,  433,   32,    2,   96,    1,\n",
      "        120,  241,   28,   50, 7701,   20,    2,  986, 1927,  805,  283,\n",
      "         21,   38,  986,  100,   31,   18,  281,   42,   10,   17,  726,\n",
      "          1,  124,    3,  356, 5812,   54,  712, 4380,    8,   47,   11,\n",
      "        254,   13,   24,   86,   19,  212,  172,    1,   55,  188, 1667,\n",
      "          6,    1,   10, 5393,  218,    2,  308,  660,   85,    2,   17,\n",
      "         76,  127, 2261,   36,    2,  442,   38,  640,    9,   41, 4300,\n",
      "        183, 1030,   16,    2,   19,    6, 1231,    1,   45,  319,    1,\n",
      "          5,    1,  475,  345,   10,  903,   11,  551,    1, 1936,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "0 --->  \n",
      "1 --->  [UNK]\n",
      "1 --->  the\n",
      "4381 ---> judy\n",
      "1 ---> [UNK]\n",
      "3524 ---> struck\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "def vectorize_text(text, label):\n",
    "    # text = tf.expand_dims(text, -1) \n",
    "    # print(f\"Input text shape: {text.shape}\")\n",
    "    return vectorization_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch, label_batch\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", first_label)\n",
    "vectorized_review = vectorize_text(first_review, first_label)\n",
    "print(\"Vectorized review\", vectorized_review)\n",
    "\n",
    "vocab = vectorization_layer.get_vocabulary()\n",
    "print(\"0 ---> \", vocab[0])  # padding token\n",
    "print(\"1 ---> \", vocab[1])  # [UNK] token (unknown words)\n",
    "print(\"1 ---> \", vocab[2])\n",
    "vectorized_sequence = vectorized_review[0].numpy().squeeze()  # Remove extra dimensions\n",
    "print(f\"{vectorized_sequence[0]} ---> {vocab[vectorized_sequence[0]]}\")\n",
    "print(f\"{vectorized_sequence[1]} ---> {vocab[vectorized_sequence[1]]}\")\n",
    "print(f\"{vectorized_sequence[2]} ---> {vocab[vectorized_sequence[2]]}\")\n",
    "print('Vocabulary size: {}'.format(len(vectorization_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Michael Keaton is \"Johnny Dangerously\" in this take-off on gangster movies done in 1984. Maureen Stapleton plays his sickly mother, Griffin Dunne is his DA brother, Peter Boyle is his boss, and Marilu Henner is his girlfriend. Other stars include Danny DeVito and Joe Piscopo. Keaton plays a pet store owner in the 1930s who catches a kid stealing a puppy and then tells him, in flashback, how he came to own the pet store. He turned to thievery at a young age to get his mother a pancreas operation ($49.95, special this week) and began working for a mob boss (Boyle). Johnny uses the last name \"Dangerously\" in the mobster world.<br /><br />There are some hilarious scenes in this film, and Stapleton is a riot as Johnny\\'s foul-mouthed mother who needs ever organ in her body replaced. Peter Boyle as Johnny\\'s boss gives a very funny performance, as does Griffin Dunne, a straight arrow DA who won\\'t \"play ball\" with crooked Burr (Danny De Vito). As Johnny\\'s nemesis, Joe Piscopo is great. Richard Dimitri is a standout as Moronie, who tortures the English language - but you have to hear him do it rather than read about it. What makes it funny is that he does it all with an angry face.<br /><br />The movie gets a little tired toward the end, but it\\'s well worth seeing, and Keaton is terrific as good boy/bad boy Johnny. For some reason, this film was underrated when it was released, and like Keaton\\'s other gem, \"Night Shift,\" you don\\'t hear much about it today. With some performances and scenes that are real gems, you\\'ll find \"Johnny Dangerously\" immensely enjoyable.', shape=(), dtype=string)\n",
      "Labels:  1\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in raw_train_ds.take(1):  # Take one batch of the dataset\n",
    "    print(X_batch)\n",
    "    print(\"Labels: \", y_batch.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_tfds(batch_size=16):\n",
    "    dataset, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "    train_ds, val_test_ds = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "    val_test_size = sum(1 for _ in val_test_ds)\n",
    "\n",
    "    test_size = int(0.5 * val_test_size)\n",
    "\n",
    "    test_ds = val_test_ds.take(test_size)\n",
    "    val_ds = val_test_ds.skip(test_size)\n",
    "\n",
    "    train_ds = train_ds.map(vectorize_text)\n",
    "    val_ds = val_ds.map(vectorize_text)\n",
    "    test_ds = test_ds.map(vectorize_text)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tfds=False, batch_size=16):\n",
    "    if tfds:\n",
    "        return load_dataset_with_tfds(batch_size)\n",
    "    else:\n",
    "        train_ds = raw_train_ds.map(vectorize_text)\n",
    "        valid_ds = raw_valid_ds.map(vectorize_text)\n",
    "        test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "        train_ds = train_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        valid_ds = valid_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        test_ds = test_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_padding = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_padding, axis=-1, keepdims=True)\n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
    "\n",
    "def build_model(type=0, max_features=10000, embedding_dim=64, network_width_factor=1, l2_reg=0.01, learning_rate=1e-4):\n",
    "    model = tf.keras.Sequential([\n",
    "        keras.layers.Embedding(max_features, embedding_dim, mask_zero=True, \n",
    "                               embeddings_regularizer=keras.regularizers.l2(l2_reg)),\n",
    "\n",
    "        keras.layers.Conv1D(network_width_factor * 8, 5, kernel_regularizer=keras.regularizers.l2(l2_reg)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        keras.layers.Conv1D(network_width_factor * 16, 5, kernel_regularizer=keras.regularizers.l2(l2_reg)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        keras.layers.Conv1D(network_width_factor * 32, 5, kernel_regularizer=keras.regularizers.l2(l2_reg)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        keras.layers.GlobalAveragePooling1D() if type == 0 else keras.layers.Lambda(compute_mean_embedding),\n",
    "\n",
    "        keras.layers.Dense(network_width_factor * 32, kernel_regularizer=keras.regularizers.l2(l2_reg)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "\n",
    "        keras.layers.Dense(1, activation='sigmoid')])\n",
    "    \n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=learning_rate, )  \n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# note on GlobalAveragePooling1D: \n",
    "\n",
    "# Purpose: This layer takes the average of the embeddings across the \n",
    "# sequence dimension. Instead of feeding each word's embedding individually \n",
    "# into the dense layers, it computes a single vector by averaging the \n",
    "# embeddings of all words in the review. Hence, you for embedding_dim 16\n",
    "# you get a single 16-dimensional vector for each review.\n",
    "\n",
    "# Why: This global average pooling effectively reduces the 1D sequence \n",
    "# of word embeddings into a single fixed-size vector, which represents \n",
    "# the overall sentiment or meaning of the entire review.\n",
    "\n",
    "# Example: If a sequence is represented as a 250x16 matrix (250 words, \n",
    "# each with a 16-dimensional embedding), this layer will compute a single \n",
    "# 16-dimensional vector by averaging the embeddings across the 250 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_path():\n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\ml\\my_env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'conv1d_12' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 32ms/step - accuracy: 0.5197 - loss: 4.6971 - val_accuracy: 0.7117 - val_loss: 2.1691 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 38ms/step - accuracy: 0.7344 - loss: 1.8792 - val_accuracy: 0.8239 - val_loss: 1.2649 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.8509 - loss: 1.1536 - val_accuracy: 0.8211 - val_loss: 1.0089 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.8909 - loss: 0.8590 - val_accuracy: 0.7983 - val_loss: 0.9505 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 32ms/step - accuracy: 0.9321 - loss: 0.6758 - val_accuracy: 0.8078 - val_loss: 0.9491 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 35ms/step - accuracy: 0.9555 - loss: 0.5568 - val_accuracy: 0.7405 - val_loss: 1.3227 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 38ms/step - accuracy: 0.9608 - loss: 0.4972 - val_accuracy: 0.8071 - val_loss: 0.9909 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 37ms/step - accuracy: 0.9700 - loss: 0.4528 - val_accuracy: 0.7956 - val_loss: 1.0584 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 37ms/step - accuracy: 0.9681 - loss: 0.4302 - val_accuracy: 0.7948 - val_loss: 1.1587 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.9691 - loss: 0.4152 - val_accuracy: 0.8079 - val_loss: 0.9973 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.9782 - loss: 0.3758 - val_accuracy: 0.8079 - val_loss: 0.9828 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.9962 - loss: 0.2978 - val_accuracy: 0.8112 - val_loss: 1.0139 - learning_rate: 5.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.9966 - loss: 0.2533 - val_accuracy: 0.8045 - val_loss: 1.0826 - learning_rate: 5.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 38ms/step - accuracy: 0.9851 - loss: 0.2625 - val_accuracy: 0.8034 - val_loss: 1.1186 - learning_rate: 5.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 36ms/step - accuracy: 0.9846 - loss: 0.2635 - val_accuracy: 0.8048 - val_loss: 1.1029 - learning_rate: 5.0000e-05\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.7809 - loss: 1.0066\n",
      "imdb_global_average_pooling_tfds.keras Accuracy: 0.8198999762535095 Loss: 0.9075161218643188\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\ml\\my_env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'conv1d_15' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - accuracy: 0.5493 - loss: 4.7547 - val_accuracy: 0.7908 - val_loss: 2.1879 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.8087 - loss: 1.9204 - val_accuracy: 0.8324 - val_loss: 1.3626 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.8738 - loss: 1.2153 - val_accuracy: 0.8355 - val_loss: 1.0782 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9073 - loss: 0.9072 - val_accuracy: 0.8211 - val_loss: 0.9867 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9407 - loss: 0.7203 - val_accuracy: 0.8074 - val_loss: 1.0109 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9591 - loss: 0.5979 - val_accuracy: 0.8093 - val_loss: 0.9945 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9653 - loss: 0.5327 - val_accuracy: 0.8127 - val_loss: 1.0226 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.9693 - loss: 0.4851 - val_accuracy: 0.8045 - val_loss: 1.0311 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9718 - loss: 0.4530 - val_accuracy: 0.7934 - val_loss: 1.1651 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 28ms/step - accuracy: 0.9778 - loss: 0.4164 - val_accuracy: 0.8202 - val_loss: 0.9746 - learning_rate: 5.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 28ms/step - accuracy: 0.9957 - loss: 0.3358 - val_accuracy: 0.8197 - val_loss: 0.9845 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.9969 - loss: 0.2885 - val_accuracy: 0.8122 - val_loss: 1.0918 - learning_rate: 5.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.9879 - loss: 0.2884 - val_accuracy: 0.8094 - val_loss: 1.1106 - learning_rate: 5.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 27ms/step - accuracy: 0.9867 - loss: 0.2868 - val_accuracy: 0.8042 - val_loss: 1.1581 - learning_rate: 5.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 29ms/step - accuracy: 0.9913 - loss: 0.2644 - val_accuracy: 0.8111 - val_loss: 1.1030 - learning_rate: 5.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.9935 - loss: 0.2489 - val_accuracy: 0.8165 - val_loss: 1.0871 - learning_rate: 2.5000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.2225 - val_accuracy: 0.8156 - val_loss: 1.0897 - learning_rate: 2.5000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.9998 - loss: 0.2011 - val_accuracy: 0.8102 - val_loss: 1.1414 - learning_rate: 2.5000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9985 - loss: 0.1848 - val_accuracy: 0.7924 - val_loss: 1.3545 - learning_rate: 2.5000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 38ms/step - accuracy: 0.9965 - loss: 0.1850 - val_accuracy: 0.8024 - val_loss: 1.2371 - learning_rate: 2.5000e-05\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.8261 - loss: 0.9499\n",
      "imdb_global_average_pooling_manual.keras Accuracy: 0.826960027217865 Loss: 0.9428495168685913\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\ml\\my_env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'conv1d_18' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 60ms/step - accuracy: 0.5446 - loss: 5.0869 - val_accuracy: 0.7729 - val_loss: 2.5883 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.7885 - loss: 2.2561 - val_accuracy: 0.8112 - val_loss: 1.5542 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 29ms/step - accuracy: 0.8586 - loss: 1.3641 - val_accuracy: 0.8249 - val_loss: 1.1663 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.9018 - loss: 0.9846 - val_accuracy: 0.8143 - val_loss: 1.0482 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 24ms/step - accuracy: 0.9312 - loss: 0.7709 - val_accuracy: 0.8178 - val_loss: 0.9612 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 29ms/step - accuracy: 0.9540 - loss: 0.6363 - val_accuracy: 0.8034 - val_loss: 1.0496 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9571 - loss: 0.5713 - val_accuracy: 0.8092 - val_loss: 1.0067 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9648 - loss: 0.5173 - val_accuracy: 0.7596 - val_loss: 1.3579 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.9659 - loss: 0.4856 - val_accuracy: 0.8005 - val_loss: 1.0568 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 29ms/step - accuracy: 0.9691 - loss: 0.4535 - val_accuracy: 0.7882 - val_loss: 1.1856 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9789 - loss: 0.4116 - val_accuracy: 0.8163 - val_loss: 0.9640 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9948 - loss: 0.3372 - val_accuracy: 0.7891 - val_loss: 1.1922 - learning_rate: 5.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 30ms/step - accuracy: 0.9960 - loss: 0.2903 - val_accuracy: 0.8089 - val_loss: 1.0402 - learning_rate: 5.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 29ms/step - accuracy: 0.9867 - loss: 0.2894 - val_accuracy: 0.7893 - val_loss: 1.2633 - learning_rate: 5.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 29ms/step - accuracy: 0.9849 - loss: 0.2893 - val_accuracy: 0.8079 - val_loss: 1.1092 - learning_rate: 5.0000e-05\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.8404 - loss: 0.9225\n",
      "imdb_early_mean_embedding_tfds.keras Accuracy: 0.826200008392334 Loss: 0.9521739482879639\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\ml\\my_env\\lib\\site-packages\\keras\\src\\layers\\layer.py:934: UserWarning: Layer 'conv1d_21' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 26ms/step - accuracy: 0.5667 - loss: 4.7602 - val_accuracy: 0.7838 - val_loss: 2.1803 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.8047 - loss: 1.8930 - val_accuracy: 0.8213 - val_loss: 1.3217 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.8650 - loss: 1.1665 - val_accuracy: 0.8296 - val_loss: 1.0259 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9060 - loss: 0.8595 - val_accuracy: 0.8273 - val_loss: 0.9455 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9390 - loss: 0.6796 - val_accuracy: 0.7583 - val_loss: 1.2360 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9570 - loss: 0.5656 - val_accuracy: 0.8028 - val_loss: 1.0211 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9681 - loss: 0.4967 - val_accuracy: 0.7850 - val_loss: 1.2233 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9682 - loss: 0.4619 - val_accuracy: 0.7970 - val_loss: 1.0691 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.9717 - loss: 0.4343 - val_accuracy: 0.7642 - val_loss: 1.3857 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9781 - loss: 0.3941 - val_accuracy: 0.8101 - val_loss: 0.9852 - learning_rate: 5.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.9963 - loss: 0.3158 - val_accuracy: 0.8225 - val_loss: 0.9340 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.9976 - loss: 0.2651 - val_accuracy: 0.8120 - val_loss: 1.0325 - learning_rate: 5.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 20ms/step - accuracy: 0.9878 - loss: 0.2682 - val_accuracy: 0.8136 - val_loss: 1.0281 - learning_rate: 5.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 19ms/step - accuracy: 0.9880 - loss: 0.2676 - val_accuracy: 0.8138 - val_loss: 1.0524 - learning_rate: 5.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - accuracy: 0.9903 - loss: 0.2521 - val_accuracy: 0.8113 - val_loss: 1.0799 - learning_rate: 5.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 24ms/step - accuracy: 0.9913 - loss: 0.2374 - val_accuracy: 0.8048 - val_loss: 1.1054 - learning_rate: 5.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - accuracy: 0.9919 - loss: 0.2277 - val_accuracy: 0.7942 - val_loss: 1.2637 - learning_rate: 2.5000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - accuracy: 0.9988 - loss: 0.2005 - val_accuracy: 0.8114 - val_loss: 1.1005 - learning_rate: 2.5000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 24ms/step - accuracy: 0.9994 - loss: 0.1814 - val_accuracy: 0.8052 - val_loss: 1.1285 - learning_rate: 2.5000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 24ms/step - accuracy: 0.9981 - loss: 0.1697 - val_accuracy: 0.8022 - val_loss: 1.2523 - learning_rate: 2.5000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 24ms/step - accuracy: 0.9951 - loss: 0.1731 - val_accuracy: 0.8042 - val_loss: 1.2467 - learning_rate: 2.5000e-05\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8268 - loss: 0.9306\n",
      "imdb_early_mean_embedding_manual.keras Accuracy: 0.8248800039291382 Loss: 0.9450625777244568\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "run_log_dir = get_log_path()\n",
    "embedding_dim = 64\n",
    "learning_rate = 1e-4\n",
    "l2_reg = 0.02\n",
    "network_width_factor = 3\n",
    "batch_size = 16\n",
    "early_stopping_patience = 10\n",
    "lr_scheduler_factor = 0.5\n",
    "lr_scheduler_patience = 5\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        j = bool(j)\n",
    "        train_ds, valid_ds, test_ds = prepare_datasets(tfds=j, batch_size=batch_size)   \n",
    "\n",
    "        if i == 0:\n",
    "            model_name = \"imdb_global_average_pooling\"\n",
    "        else:\n",
    "            model_name = \"imdb_early_mean_embedding\"\n",
    "\n",
    "        if j == 0:\n",
    "            model_name += \"_tfds.keras\"\n",
    "        else:\n",
    "            model_name += \"_manual.keras\"\n",
    "\n",
    "        model_logdir = f\"{run_log_dir}/{model_name}\"\n",
    "        os.makedirs(model_logdir, exist_ok=True)\n",
    "        tensorboard_cb = keras.callbacks.TensorBoard(model_logdir)\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "        lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=lr_scheduler_factor, patience=lr_scheduler_patience)\n",
    "\n",
    "        model = build_model(i)\n",
    "        model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[tensorboard_cb, early_stopping_cb, lr_scheduler])\n",
    "\n",
    "        model.save(f\"{model_logdir}/{model_name}\")\n",
    "\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        print(f\"{model_name} Accuracy: {accuracy} Loss: {loss}\")\n",
    "\n",
    "        with open(f\"{run_log_dir}/results.txt\", \"a\") as f:\n",
    "            f.write(f\"{model_name} Accuracy: {accuracy} Loss: {loss}\\n\")\n",
    "\n",
    "    \n",
    "with open(f\"{run_log_dir}/results.txt\", \"a\") as f:\n",
    "    f.write(f\"\\n---------------------------\\n\")\n",
    "    f.write(f\"NETWORK PARAMETERS:\\n\")\n",
    "    f.write(f\"---------------------------\\n\")\n",
    "    f.write(f\"No. of epochs: {epochs}\\n\")\n",
    "    f.write(f\"Embedding dim: {embedding_dim}\\n\")\n",
    "    f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    f.write(f\"L2 reg: {l2_reg}\\n\")\n",
    "    f.write(f\"Network width factor: {network_width_factor}\\n\")\n",
    "    f.write(f\"Batch size: {batch_size}\\n\")\n",
    "    f.write(f\"Early stopping patience: {early_stopping_patience}\\n\")\n",
    "    f.write(f\"LR scheduler factor: {lr_scheduler_factor}\\n\")\n",
    "    f.write(f\"LR scheduler patience: {lr_scheduler_patience}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
