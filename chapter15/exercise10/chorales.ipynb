{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mido\n",
    "import pygame.midi\n",
    "from mido import MidiFile, MidiTrack, Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n",
    "FILENAME = \"jsb_chorales.tgz\"\n",
    "filepath = keras.utils.get_file(FILENAME,\n",
    "                                DOWNLOAD_ROOT + FILENAME,\n",
    "                                cache_subdir=\"datasets/jsb_chorales\",\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)\n",
    "\n",
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_chorale(chorale_array, tempo=100000):\n",
    "    \"\"\"\n",
    "    Play a Bach chorale using the given array of notes.\n",
    "    \n",
    "    Parameters:\n",
    "    chorale_array (numpy array): A 2D array where each row is a time step and each column is a note index.\n",
    "    tempo (int): Tempo of the MIDI playback, default is 500000 (microseconds per beat).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the MIDI file\n",
    "    mid = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    # Set the tempo (microseconds per beat)\n",
    "    track.append(mido.MetaMessage('set_tempo', tempo=tempo))\n",
    "\n",
    "    # MIDI note on/off settings\n",
    "    note_on = 144  # MIDI message for note on\n",
    "    note_off = 128  # MIDI message for note off\n",
    "\n",
    "    for time_step in chorale_array:\n",
    "        # For each time step (row), send note-on messages for the active notes\n",
    "        for note in time_step:\n",
    "            if note != 0:  # If the note is not 0 (0 means no note is played)\n",
    "                track.append(Message('note_on', note=note, velocity=64, time=0))\n",
    "        \n",
    "        # Duration of each time step (you can adjust this)\n",
    "        time_per_step = 480  # time per step in MIDI ticks (adjustable)\n",
    "        \n",
    "        # Send note-off messages after the duration\n",
    "        for note in time_step:\n",
    "            if note != 0:\n",
    "                track.append(Message('note_off', note=note, velocity=64, time=time_per_step))\n",
    "    \n",
    "    # Save the generated MIDI to a file\n",
    "    midi_filename = \"bach_chorale.mid\"\n",
    "    mid.save(midi_filename)\n",
    "    print(f\"Chorale saved as {midi_filename}\")\n",
    "    \n",
    "    # Initialize pygame for MIDI playback\n",
    "    pygame.midi.init()\n",
    "\n",
    "    # Manually select Microsoft GS Wavetable Synth (device 1)\n",
    "    output_device_id = 1  # Based on your device listing, use Device 1\n",
    "    \n",
    "    if output_device_id >= 0:\n",
    "        player = pygame.midi.Output(output_device_id)\n",
    "    else:\n",
    "        print(\"No valid MIDI output device found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Set instrument to Acoustic Grand Piano (General MIDI instrument 0)\n",
    "        player.set_instrument(0)\n",
    "        \n",
    "        # Parse the MIDI file using mido and send the MIDI messages to the player\n",
    "        for msg in mido.MidiFile(midi_filename).play():\n",
    "            if not msg.is_meta:\n",
    "                if msg.type == 'note_on':\n",
    "                    player.note_on(msg.note, msg.velocity)\n",
    "                elif msg.type == 'note_off':\n",
    "                    player.note_off(msg.note, msg.velocity)\n",
    "        \n",
    "        print(\"Playing chorale...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error playing MIDI: {e}\")\n",
    "    finally:\n",
    "        player.close()\n",
    "        pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))\n",
      "(16, 131) (16, 131)\n",
      "tf.Tensor(\n",
      "[39 35 30 23 39 35 30 23 39 35 30 23 39 35 30 23 40 35 23 20 40 35 23 20\n",
      " 40 35 25 20 40 35 25 20 42 34 27 15 42 34 27 15 42 34 27 15 42 34 27 15\n",
      " 42 35 27 20 42 35 27 20 42 34 27 20 42 34 27 20 40 32 28 13 40 32 28 13\n",
      " 40 34 28 13 40 34 28 13 39 35 30 11 39 35 30 11 39 35 30 11 39 35 30 11\n",
      " 37 34 30 18 37 34 30 18 37 34 30 18 37 34 30 18 37 34 30 18 37 34 30 18\n",
      " 37 34 30 18 37 34 30 18 39 35 30], shape=(131,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[35 30 23 39 35 30 23 39 35 30 23 39 35 30 23 40 35 23 20 40 35 23 20 40\n",
      " 35 25 20 40 35 25 20 42 34 27 15 42 34 27 15 42 34 27 15 42 34 27 15 42\n",
      " 35 27 20 42 35 27 20 42 34 27 20 42 34 27 20 40 32 28 13 40 32 28 13 40\n",
      " 34 28 13 40 34 28 13 39 35 30 11 39 35 30 11 39 35 30 11 39 35 30 11 37\n",
      " 34 30 18 37 34 30 18 37 34 30 18 37 34 30 18 37 34 30 18 37 34 30 18 37\n",
      " 34 30 18 37 34 30 18 39 35 30 11], shape=(131,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:]\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1)\n",
    "    return tf.reshape(window, [-1])\n",
    "\n",
    "def load_chorales_dataset(files, batch_size=16, shuffle_buffer_size=None, \n",
    "                 window_size=32, window_shift=8, cache=True):\n",
    "    \n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "    \n",
    "    csv_files = glob.glob(files + '/*.csv')\n",
    "    chorales_list = [pd.read_csv(f, header=0) for f in csv_files]\n",
    "    chorales_list = [np.array(chorale) for chorale in chorales_list]\n",
    "\n",
    "    chorales_dataset = tf.ragged.constant(chorales_list, ragged_rank=1)\n",
    "    chorales_dataset = tf.data.Dataset.from_tensor_slices(chorales_dataset)\n",
    "    chorales_dataset = chorales_dataset.flat_map(to_windows)\n",
    "    chorales_dataset = chorales_dataset.map(preprocess)\n",
    "    \n",
    "    if cache:\n",
    "        chorales_dataset = chorales_dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        chorales_dataset = chorales_dataset.shuffle(shuffle_buffer_size)\n",
    "    chorales_dataset = chorales_dataset.batch(batch_size)\n",
    "    chorales_dataset = chorales_dataset.map(create_target)\n",
    "\n",
    "    return chorales_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train = load_chorales_dataset('jsb_chorales/train')\n",
    "valid = load_chorales_dataset('jsb_chorales/valid')\n",
    "test = load_chorales_dataset('jsb_chorales/test')\n",
    "\n",
    "print(train.element_spec)\n",
    "print(valid.element_spec)\n",
    "print(test.element_spec)\n",
    "\n",
    "for X_batch, Y_batch in train.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)\n",
    "    print(X_batch[0])\n",
    "    print(Y_batch[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\ml\\my_env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_embedding_dims = 10\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.GRU(256, return_sequences=True),\n",
    "    keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir():\n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    380/Unknown \u001b[1m47s\u001b[0m 110ms/step - accuracy: 0.6228 - loss: 1.4756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48503\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 126ms/step - accuracy: 0.6230 - loss: 1.4746 - val_accuracy: 0.7637 - val_loss: 0.8694 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 154ms/step - accuracy: 0.7206 - loss: 1.0676 - val_accuracy: 0.1080 - val_loss: 4.1395 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 212ms/step - accuracy: 0.1813 - loss: 3.3299 - val_accuracy: 0.1974 - val_loss: 3.1370 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 186ms/step - accuracy: 0.2226 - loss: 3.0002 - val_accuracy: 0.2237 - val_loss: 2.9716 - learning_rate: 0.0075\n",
      "Epoch 5/20\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 154ms/step - accuracy: 0.2311 - loss: 2.9473 - val_accuracy: 0.2339 - val_loss: 2.9286 - learning_rate: 0.0075\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.75, patience=2)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "history = model.fit(train, epochs=20, validation_data=valid, callbacks=[early_stopping_cb, lr_scheduler, tensorboard_cb])\n",
    "\n",
    "model.save('bach_chorales_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(sequence):\n",
    "    sequence = tf.where(sequence == 0, 0, sequence + min_note - 1)\n",
    "    sequence = tf.cast(sequence, tf.int32)\n",
    "    return tf.reshape(sequence, [-1, 4]).numpy()\n",
    "\n",
    "def generate_chorale(model, seed_chords, length):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            #next_note = model.predict_classes(arpegio)[:1, -1:]\n",
    "            next_note = np.argmax(model.predict(arpegio), axis=-1)[:1, -1:]\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])\n",
    "\n",
    "def generate_chorale_v2(model, seed_chords, length, temperature=1):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note_probas = model.predict(arpegio, verbose=False)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65, 60, 57, 53], [65, 60, 57, 53], [65, 60, 57, 53], [65, 60, 57, 53], [72, 60, 55, 52], [72, 60, 55, 52], [70, 60, 55, 52], [70, 60, 55, 52], [69, 60, 53, 53], [69, 60, 53, 53], [67, 60, 55, 52], [67, 60, 55, 52]]\n",
      "[65 60 57 53]\n",
      "[65 60 57 53]\n",
      "[65 60 57 53]\n",
      "[65 60 57 53]\n",
      "[72 60 55 52]\n",
      "[72 60 55 52]\n",
      "[70 60 55 52]\n",
      "[70 60 55 52]\n",
      "[69 60 53 53]\n",
      "[69 60 53 53]\n",
      "[67 60 55 52]\n",
      "[67 60 55 52]\n",
      "[67 60 57 52]\n",
      "[67 60 57 52]\n",
      "[69 60 53 53]\n",
      "[69 60 53 53]\n",
      "[69 60 53 53]\n",
      "[69 60 53 53]\n",
      "[67 60 55 48]\n",
      "[67 60 55 48]\n",
      "[67 60 55 48]\n",
      "[67 60 55 48]\n",
      "[65 60 57 45]\n",
      "[65 60 57 45]\n",
      "[65 60 57 45]\n",
      "[65 60 57 45]\n",
      "[65 62 58 46]\n",
      "[65 62 58 46]\n",
      "[65 62 58 46]\n",
      "[65 62 58 46]\n",
      "[67 62 58 46]\n",
      "[67 62 58 46]\n",
      "[67 62 58 46]\n",
      "[67 62 58 46]\n",
      "[69 63 57 45]\n",
      "[69 63 57 45]\n",
      "[69 63 57 45]\n",
      "[69 63 57 45]\n",
      "[69 62 57 50]\n",
      "[69 62 57 50]\n",
      "[69 62 57 50]\n",
      "[69 62 57 50]\n",
      "[67 62 55 50]\n",
      "[67 62 55 50]\n",
      "[67 62 55 50]\n",
      "[67 62 55 50]\n",
      "[69 64 55 48]\n",
      "[69 64 55 48]\n",
      "[69 64 55 48]\n",
      "[69 64 55 48]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "[69 62 54 50]\n",
      "Chorale saved as bach_chorale.mid\n",
      "Playing chorale...\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('bach_chorales_model.keras')\n",
    "\n",
    "seed_chords = test_chorales[0][:12]\n",
    "\n",
    "print(seed_chords)\n",
    "\n",
    "\n",
    "generated_chorale = generate_chorale_v2(model, seed_chords, 64, 0.8)\n",
    "# generated_chorale = postprocess(generated_chorale)\n",
    "\n",
    "for chord in generated_chorale:\n",
    "    print(chord)\n",
    "\n",
    "# Play the processed chorale (assuming play_chorale is already defined)\n",
    "play_chorale(generated_chorale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
