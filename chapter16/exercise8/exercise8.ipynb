{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reberGrammarInner():\n",
    "    embeddedReberGrammar = [\n",
    "        [('B', 1)],           # (state 0) =B=>(state 1)\n",
    "        [('T', 2), ('P', 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "        [('S', 2), ('X', 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "        [('T', 3), ('V', 5)], # (state 3) =T=>(state 3) or =V=>(state 5)\n",
    "        [('X', 3), ('S', 6)], # (state 4) =X=>(state 3) or =S=>(state 6)\n",
    "        [('P', 4), ('V', 6)], # (state 5) =P=>(state 4) or =V=>(state 6)\n",
    "        [('E', None)]         # (state 6) =E=>(terminal)\n",
    "    ]\n",
    "    state = 0\n",
    "    reberString = ''\n",
    "    while state is not None:\n",
    "        char, nextState = embeddedReberGrammar[state][np.random.randint(len(embeddedReberGrammar[state]))]\n",
    "        reberString += char\n",
    "        state = nextState\n",
    "    return reberString\n",
    "\n",
    "def reberGrammarOuter(innerString):\n",
    "    embeddedReberGrammar = [\n",
    "        [('B', 1)],           # (state 0) =B=>(state 1)\n",
    "        [('T', 2), ('P', 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "        [('T', 4)],            # (state 2) =T=>(state 4)\n",
    "        [('P', 4)],            # (state 3) =P=>(state 4)\n",
    "        [('E', None)]         # (state 4) =E=>(terminal)\n",
    "    ]\n",
    "    state = 0\n",
    "    reberString = ''\n",
    "    while state is not None:\n",
    "        if state == 2 or state == 3:\n",
    "            reberString += innerString\n",
    "        char, nextState = embeddedReberGrammar[state][np.random.randint(len(embeddedReberGrammar[state]))]\n",
    "        reberString += char\n",
    "        state = nextState\n",
    "    return reberString\n",
    "\n",
    "def validateInnerReberGrammar(innerString):\n",
    "    embeddedReberGrammar = [\n",
    "        [('B', 1)],           # (state 0) =B=>(state 1)\n",
    "        [('T', 2), ('P', 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "        [('S', 2), ('X', 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "        [('T', 3), ('V', 5)], # (state 3) =T=>(state 3) or =V=>(state 5)\n",
    "        [('X', 3), ('S', 6)], # (state 4) =X=>(state 3) or =S=>(state 6)\n",
    "        [('P', 4), ('V', 6)], # (state 5) =P=>(state 4) or =V=>(state 6)\n",
    "        [('E', None)]         # (state 6) =E=>(terminal)\n",
    "    ]\n",
    "    state = 0\n",
    "    for char in innerString:\n",
    "        found = False\n",
    "        for c, nextState in embeddedReberGrammar[state]:\n",
    "            if char == c:\n",
    "                state = nextState\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def vaidateOuterReberGrammar(outerString):\n",
    "    embeddedReberGrammar = [\n",
    "        [('B', 1)],           # (state 0) =B=>(state 1)\n",
    "        [('T', 2), ('P', 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "        [('T', 4)],            # (state 2) =T=>(state 4)\n",
    "        [('P', 4)],            # (state 3) =P=>(state 4)\n",
    "        [('E', None)]         # (state 4) =E=>(terminal)\n",
    "    ]\n",
    "    state = 0\n",
    "    for char in outerString:\n",
    "        found = False\n",
    "        for c, nextState in embeddedReberGrammar[state]:\n",
    "            if char == c:\n",
    "                state = nextState\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def validateReberGrammar(reberString):\n",
    "    innerReber = reberString[2:-2]\n",
    "    outerReber = reberString[:2] + reberString[-2:]\n",
    "\n",
    "    return validateInnerReberGrammar(innerReber) and vaidateOuterReberGrammar(outerReber)\n",
    "\n",
    "def generateCorruptedGrammar():\n",
    "    generated_string = reberGrammarOuter(reberGrammarInner())\n",
    "    # print(\"Good String:     \", generated_string)\n",
    "    while validateReberGrammar(generated_string):\n",
    "        good_list = list(generated_string)\n",
    "        idx = np.random.randint(len(good_list))\n",
    "        good_list[idx] = np.random.choice([c for c in 'BTPSXVE'])\n",
    "        generated_string = ''.join(good_list)\n",
    "    # print(\"Corrupted String:\", generated_string)\n",
    "    return generated_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(size, max_length=30, easier_variant=False):\n",
    "    # half of the dataset is composed of strings that follow the ERG rules\n",
    "    string_sequences = [reberGrammarOuter(reberGrammarInner()) for _ in range(size // 2)]\n",
    "    labels = [1.] * (size // 2)\n",
    "\n",
    "    if easier_variant:\n",
    "        standard_deviation = 0\n",
    "        list_of_lengths = []\n",
    "        for i in range(100000):\n",
    "            genStringLen = len(reberGrammarOuter(reberGrammarInner()))\n",
    "            list_of_lengths.append(genStringLen)\n",
    "\n",
    "        mean = np.mean(list_of_lengths)\n",
    "        print(f\"Mean Length of ERG String: {mean}\")\n",
    "\n",
    "        for i in range(100000):\n",
    "            standard_deviation += (list_of_lengths[i] - mean)**2\n",
    "\n",
    "        standard_deviation = np.sqrt(standard_deviation/100000)\n",
    "        \n",
    "        # one eight is composed of random strings\n",
    "        for _ in range(size // 8):\n",
    "            length = int(np.random.normal(mean, standard_deviation))\n",
    "            reberString = ''\n",
    "            while len(reberString) < length:\n",
    "                reberString += np.random.choice(['B', 'E', 'T', 'P', 'S', 'X', 'V'])\n",
    "            string_sequences.append(reberString)\n",
    "        labels += [0.] * (size // 8)\n",
    "\n",
    "        # one eight is composed of inner RG strings\n",
    "        for _ in range(size // 8):\n",
    "            reberString = reberGrammarInner()\n",
    "            string_sequences.append(reberString)\n",
    "        labels += [0.] * (size // 8)\n",
    "\n",
    "        # one fourth is composed of random strings wrapped in outer RG strings\n",
    "        for _ in range(size // 4):\n",
    "            length = int(np.random.normal(mean, standard_deviation)) - 4 # 4 is the length added by the outer RG\n",
    "            reberString = ''\n",
    "            while len(reberString) < length:\n",
    "                reberString += np.random.choice(['B', 'E', 'T', 'P', 'S', 'X', 'V'])\n",
    "            string_sequences.append(reberGrammarOuter(reberString))\n",
    "        labels += [0.] * (size // 4)\n",
    "    else:\n",
    "        # half of the dataset is composed of corrupted strings\n",
    "        for _ in range(size // 2):\n",
    "            string_sequences.append(generateCorruptedGrammar())\n",
    "        labels += [0.] * (size // 2)\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, )\n",
    "    tokenizer.fit_on_texts(string_sequences)\n",
    "    string_sequences = tokenizer.texts_to_sequences(string_sequences)\n",
    "    string_sequences = tf.keras.preprocessing.sequence.pad_sequences(string_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "    features = tf.constant(string_sequences)\n",
    "    labels = tf.constant(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "    return dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.6477 - loss: 0.6525 - val_accuracy: 0.2280 - val_loss: 1.0157\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.7243 - loss: 0.5754 - val_accuracy: 0.3620 - val_loss: 0.8744\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.8212 - loss: 0.4312 - val_accuracy: 0.7160 - val_loss: 0.5558\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9325 - loss: 0.2050 - val_accuracy: 0.8250 - val_loss: 0.4651\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9663 - loss: 0.1255 - val_accuracy: 0.9330 - val_loss: 0.2292\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9625 - loss: 0.1317 - val_accuracy: 0.9620 - val_loss: 0.1068\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9814 - loss: 0.0736 - val_accuracy: 0.9660 - val_loss: 0.0731\n",
      "Epoch 8/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9898 - loss: 0.0373 - val_accuracy: 0.9840 - val_loss: 0.0366\n",
      "Epoch 9/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.9947 - loss: 0.0265 - val_accuracy: 0.9980 - val_loss: 0.0074\n",
      "Epoch 10/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.9955 - loss: 0.0186 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 11/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9939 - loss: 0.0247 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 12/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9975 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 13/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9987 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 7.0364e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9988 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 8.4868e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 0.9990 - val_loss: 0.0046\n",
      "Epoch 16/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9986 - loss: 0.0060 - val_accuracy: 0.9990 - val_loss: 0.0024\n",
      "Epoch 17/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9986 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 5.6522e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 0.9990 - val_loss: 0.0022\n",
      "Epoch 19/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9975 - loss: 0.0080 - val_accuracy: 0.9990 - val_loss: 0.0026\n",
      "Epoch 20/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9974 - loss: 0.0080 - val_accuracy: 0.9970 - val_loss: 0.0066\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0057\n",
      "Test Accuracy: 0.9980000257492065\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "dataset, tokenizer = generateDataset(10000, max_length)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = (\n",
    "    dataset.take(train_size)\n",
    "    .shuffle(buffer_size=train_size)  # Shuffle before batching\n",
    "    .batch(32)  # Apply batching\n",
    "    .cache()  # Cache the dataset for better performance\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    dataset.skip(train_size)\n",
    "    .take(val_size)\n",
    "    .batch(32)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    dataset.skip(train_size + val_size)\n",
    "    .take(test_size)\n",
    "    .batch(32)\n",
    ")\n",
    "\n",
    "vocab_size = len(['B','E', 'T', 'P', 'S', 'X', 'V']) + 1\n",
    "embedding_dim = 32\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "    tf.keras.layers.GRU(64, return_sequences=True),\n",
    "    tf.keras.layers.GRU(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=20, validation_data=val_dataset)\n",
    "\n",
    "accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "Estimated probability that these are Reber strings:\n",
      "BPBTSSSSSSSSSSXXTTTTTTTTVPSETE: 8.39%\n",
      "BPBTSSSSSSSSSSXXTTTTTTTTVPSEPE: 99.99%\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\"BPBTSSSSSSSSSSXXTTTTTTTTVPSETE\", # incorrect T at the end should be P\n",
    "                \"BPBTSSSSSSSSSSXXTTTTTTTTVPSEPE\"] # correct\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test_strings)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
