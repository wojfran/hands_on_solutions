# Embedded Reber Grammar Recognition with Recurrent Neural Network

## Project Overview

This project implements a Recurrent Neural Network (RNN) to recognize and classify strings generated by an Embedded Reber Grammar (ERG). The exercise is inspired by the work of Hochreiter and Schmidhuber on Long Short-Term Memory (LSTM) networks and explores the capabilities of neural networks in learning complex grammatical patterns.

## Background

Embedded Reber Grammars are artificial languages with specific transition rules that generate strings following a precise set of grammatical constraints. In this implementation, we use a specific ERG that generates strings like "BPBTSXXVPSEPE", where each character follows a strict set of production rules.

## Key Components

### Grammar Generation Functions
- `reberGrammarInner()`: Generates strings following the inner Reber Grammar rules
- `reberGrammarOuter()`: Generates strings following the outer Reber Grammar rules
- `validateReberGrammar()`: Validates whether a generated string follows the complete grammar rules

### Dataset Generation
- Generates a balanced dataset with:
  - 50% valid Reber Grammar strings
  - 50% corrupted or invalid strings
- Supports two variants:
  1. Standard: Directly corrupting valid strings
  2. Easier variant: Generating alternative invalid string types

### Neural Network Model
- Uses a GRU-based Recurrent Neural Network
- Architecture:
  - Embedding layer
  - Two GRU layers (64 units each)
  - Dense sigmoid output layer for binary classification

## Requirements
- TensorFlow
- NumPy
- Matplotlib (optional, for visualization)

## Training Process
- Dataset split: 80% training, 10% validation, 10% testing
- Binary cross-entropy loss
- Adam optimizer
- 20 training epochs

## Performance
In the example run, the model achieved:
- Training Accuracy: ~99.9%
- Validation Accuracy: ~99.7%
- Test Accuracy: 99.8%

## Interesting Observations
- The model quickly learns to distinguish between valid and invalid Reber Grammar strings
- High accuracy demonstrates RNNs' capability to learn complex sequential patterns
- Showcases the importance of sequence-based neural network architectures

## References
- Hochreiter & Schmidhuber (1997) paper on LSTMs
- Jenny Orr's introduction to Reber Grammars

## Potential Improvements
- Experiment with LSTM layers instead of GRU
- Try different embedding dimensions
- Implement more complex grammar variations
- Visualize learned embeddings

## License
MIT License
